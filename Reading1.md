# Linear Algebra with Applications - Bretscher (2015)
## 3.1 Subspaces of ℝ<sup>n</sup> and their Dimensions
### Finding Kernel of a Matrix (AKA Null Space)
1. rref(A)
2. Write the non-pivot columns of rref(A) as their own column vectors, separated by commas, in order
   * If there are no non-pivot columns in rref(A), then the columns are independent. This means the kernel only contains the zero vector, where the number of rows equals the number of columns in A 
3. ***Multiply each of these column vectors by -1***
4. Append the identity matrix to the bottom of these column vectors such that their number of rows equals the number of columns in A
5. The resulting columns span the kernel of A
* [Example](https://www.youtube.com/watch?v=bqBacABVCeQ)
### Finding Image of a Matrix (AKA Range)
1. rref(A)
2. Note the pivot columns. Write the corresponding original columns from A as their own column vectors, separated by commas, in order
3. The resulting columns span the kernel of A
* [Example](https://www.youtube.com/watch?v=xa92zIehBZ8)
## 3.2 Subspaces of ℝ<sup>n</sup>, Bases and Linear Independence
* The pivot columns of a matrix A are linearly independent vectors, while the non-pivot columns are linearly dependent
### Prove V is a Subspace
1. V must contain the zero vector
2. V is closed under addition (for any two vectors in V, the sum of these vectors is also in V)
3. V is closed under scalar multiplication (for any vector in V multiplied by a scalar, the product is also in V)
* [Example](https://www.youtube.com/watch?v=rPF6Xk5OGU8)
### Determine if Vectors are Linearly Independent
1. Create an augmented matrix with the vectors in question and add a zero column to the right-most side
2. Determine rref of the matrix
3. If there is a leading 1 in each column, the vectors are linearly independent. Otherwise, they are linearly dependent
## 3.3 The Dimension of a Subspace of ℝ<sup>n</sup>
### Finding Dimension of a Subspace
1. Find a basis of the subspace
2. How many vectors are in the basis? The subspace has that many dimensions
* [Example](https://www.youtube.com/watch?v=kfVI7Tp98WM)
### Rank-Nullity Theorem
* For any `n x m` matrix: dim(ker A) + dim(imA) = m
* Equivalently, (nullity of A) + (rank of A) = m
## 3.4 Coordinates
* [x]<sub>B</sub> indicates that the vector x has been written with respect to the basis B, not the standard basis
### Find x given [x]<sub>B</sub>
1. Multiply each element in [x]<sub>B</sub> by its corresponding vector in the basis B
2. Sum the resulting vectors to yield x
* [Example](https://www.youtube.com/watch?v=7P_XGrb3d3c)
### Find [x]<sub>B</sub> given x
1. Create an augmented matrix with the vectors in the basis B on the left and the vector x on the right
2. Find rref of the matrix from step (1). The result in the right-most column of the augmented matrix is [x]<sub>B</sub>
* [Example](https://www.youtube.com/watch?v=4sBXY1BCU3w)
### Change of Coordinates: Find the Matrix of the Linear Transformation from Basis B to Basis A
1. Create a new matrix wherein the columns are the vectors of the starting basis, written with respect to the goal basis. Hence, columns of the new matrix are [b<sub>1</sub>]<sub>A</sub>, [b<sub>2</sub>]<sub>A</sub>, ...
2. We need to solve the augmented matrix for each [b<sub>i</sub>]<sub>A</sub>, where the vectors from the goal basis are on the left and the starting basis column [b<sub>i</sub>]<sub>A</sub> is on the right
3. Repeat step (2) for each [b<sub>i</sub>]<sub>A</sub>
4. Combine the column vectors generated by the previous steps to yield the transformation matrix
* [Example](https://www.youtube.com/watch?v=2K6ipONMIgg)
### Find the Matrix B of the Linear Transformation T(x) = Ax with Respect to the Basis (v<sub>1</sub>, ... , v<sub>m</sub>)
1. Write the basis v<sub>1</sub>, ... , v<sub>m</sub> as a single matrix with each v<sub>i</sub> being a column vector. Call it C.
2. Find C<sup>-1</sup>.
   * To find the inverse of a `2 x 2` matrix, swap the positions of a and d, put negatives in front of b and c, and divide everything by the determinant (ad-bc).
3. Multiply the matricies C<sup>-1</sup>AC. This will give us B. 
* [Example}(https://www.youtube.com/watch?v=lCRGNykWqFI)
### Similar Matricies
* Consider two `n × n` matrices A and B. We say that A is similar to B if there exists an invertible matrix S such that AS = SB, or B = S<sup>−1</sup>AS.
## 5.1 Orthogonal Projections and Orthonormal Bases
* The vectors u<sub>1</sub>, u<sub>2</sub>, ... , u<sub>m</sub> in ℝ<sub>n</sub> are called orthonormal if they are all unit vectors and orthogonal to one another: 
  * u<sub>j</sub> • u<sub>j</sub> = 1 if u<sub>j</sub> = u<sub>j</sub>
  * u<sub>j</sub> • u<sub>j</sub> = 0 if u<sub>j</sub> ≠ u<sub>j</sub>
* Orthonormal vectors are linearly independent and form a basis of ℝ<sub>n</sub>
* [Example](https://www.youtube.com/watch?v=7BFx8pt2aTQ)
## 5.2 Gram–Schmidt Process and QR Factorization
* Orthonormalize 3 vectors x<sub>1</sub>, x<sub>2</sub>, x<sub>3</sub>:
  * v<sub>1</sub> = x<sub>1</sub>
  * v<sub>2</sub> = x<sub>2</sub> - ((x<sub>2</sub> • v<sub>1</sub>) / (v<sub>1</sub> • v<sub>1</sub>))v<sub>1</sub>
  * v<sub>3</sub> = x<sub>3</sub> - ((x<sub>3</sub> • v<sub>1</sub>) / (v<sub>1</sub> • v<sub>1</sub>))v<sub>1</sub> - ((x<sub>3</sub> • v<sub>2</sub>) / (v<sub>2</sub> • v<sub>2</sub>))v<sub>2</sub>
  * At this point, we have orthogonal vectors. To get orthonormal vectors, we need to divide each v by its maginitude, ||v||. In other words, we divide each vector by the square root of the sum of its elements squared. 
  * [Example](https://www.youtube.com/watch?v=swXcm_vTjWU)
